{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenethanxd/data/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXaISb6Aly_c"
      },
      "source": [
        "# Classification\n",
        "\n",
        "Classification is a supervised machine learning method where the model predicts the correct label or category from a set of input data. The model is trained using labeled training data, and then it performs predictions on unseen data.\n",
        "\n",
        "For example:\n",
        "\n",
        "    * Customer Churn Prediction: Detecting whether a customer is likely to churn (leave the service) is a classification problem.\n",
        "    * COVID-19 Detection: During the COVID-19 pandemic, classification models were used to predict whether a person had COVID-19 based on their symptoms and test results.\n",
        "\n",
        "It's important not to confuse classification with regression, another type of supervised learning. While both involve predicting outcomes based on input data, they differ in the type of output they predict:\n",
        "\n",
        "    * Classification: Predicts discrete labels or categories (e.g., spam vs. non-spam emails, whether a tumor is malignant or benign).\n",
        "    * Regression: Predicts continuous values (e.g., predicting house prices, estimating a person's salary based on their experience).\n",
        "    \n",
        "Understanding the distinction between these methods is crucial for selecting the appropriate model and evaluation metrics for your specific problem.\n",
        "\n",
        "* In Classification, the label is from a set of categorical label values that the model learns from training. These are called discrete labels. For example, predicting whether an email is spam or not spam.\n",
        "* In Regression, the label is much wider and doesn't have any limits. It shows the relationship of the input features. These are called continuous labels. For example, predicting house prices based on various features like size, location, and number of bedrooms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNeftN3Dly_d"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Classification is a fundamental task in machine learning where the goal is to assign labels to instances based on their features. This process involves training a model on a labeled dataset so that it can predict the correct label for new, unseen instances. Classification is widely used in various applications, including spam detection, medical diagnosis, and image recognition.\n",
        "\n",
        "Several popular methods employed in classification:\n",
        "\n",
        "* Logistic Regression: A statistical model used to predict a binary outcome based on one or more predictors (input values). It is widely used for linearly separable data.\n",
        "* Support Vector Machine (SVM): This method finds a line or hyperplane that best separates the data into different classes and is usually used in high-dimensional spaces.\n",
        "* Decision Tree: These models use a tree-like graph of decisions and their possible consequences. They are easy to interpret and can handle both categorical and numerical data.\n",
        "* K-Nearest Neighbors (KNN): A non-parametric method where the class of a sample is determined by the majority class among its k-nearest neighbors in the feature space. It is simple and effective for small datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em1tsGkcly_d"
      },
      "source": [
        "## Understanding classification\n",
        "\n",
        "### Definition\n",
        "\n",
        "Classification is a supervised learning task in machine learning where the goal is to categorize instances into predefined classes based on their features. It is a crucial process because it enables the automation of decision-making and prediction processes across various fields. Classification helps in organizing and interpreting complex data, making it easier to derive actionable insights.\n",
        "\n",
        "### Types of Classification\n",
        "\n",
        "There are 3 main types of classification:\n",
        "\n",
        "* Binary Classification: Involves two classes. Examples include spam detection (spam or not spam) and disease diagnosis (disease or no disease).\n",
        "* Multiclass Classification: Involves more than two classes. Examples include digit recognition (0-9) and categorizing types of animals in images.\n",
        "* Multilabel Classification: Each instance can belong to multiple classes simultaneously. Examples include tagging multiple objects in an image or classifying news articles into multiple topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5-z4av3ly_e"
      },
      "source": [
        "## Packages\n",
        "\n",
        "In this training material, we'll be using mostly `numpy` to perform operations. We'll be also covering some extra packages that simplify the workload. It's important that we understand how the operations perform. In a nutshell, here's what you need to install:\n",
        "\n",
        "* numpy: A Python library used for working with arrays.\n",
        "* pandas: A data manipulation and analysis library.\n",
        "* scikit-learn: A library for machine learning that includes numerous classification algorithms.\n",
        "* matplotlib and seaborn: Libraries for data visualization.\n",
        "* math: A standard Python library for mathematical operations.\n",
        "\n",
        "Some models will include a NumPy version to help us understand how they work. These models are often based on mathematical formulas. A basic understanding of linear Algebra, probability, and statistics is necessary to fully grasp the concepts. You are encouraged to try your best to complete every exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWXwkwiQly_e"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSnmFRzJly_e"
      },
      "source": [
        "### K-nearest Neighbors (KNN)\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised learning algorithm that classifies new data points based on similarity to known data points. Unlike traditional models, KNN doesn't learn parameters during training; instead, it memorizes the entire training dataset. The algorithm calculates distances between the new data point and all existing ones, then assigns the new point to the class most common among its nearest neighbors (determined by a user-defined parameter, K). This simplicity makes KNN suitable for various data types and robust against outliers, though it can be computationally intensive with large datasets and sensitive to the choice of distance metric.\n",
        "\n",
        "In practice, KNN is effective for small to moderate-sized datasets where the underlying data distribution is not explicitly known. Its ability to handle both numerical and categorical data without assumptions about their distributions makes it versatile. However, its lazy-learning approach, where predictions are made only when needed, can lead to slower prediction times compared to eager-learning algorithms. Understanding these trade-offs helps in choosing KNN appropriately for tasks like image recognition, recommendation systems, and medical diagnostics where similarity-based classification is beneficial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSQXoMd9ly_e"
      },
      "source": [
        "#### Distance metrics\n",
        "\n",
        "Distance measures are essential in understanding the proximity of a feature to the center of a cluster or group. Two commonly used distance metrics are Euclidean and Manhattan distances.\n",
        "\n",
        "#### Euclidean distance\n",
        "\n",
        "This is the straight-line distance between two points in Euclidean space. It measures the shortest path between two points, which is the geometric interpretation of distance we are most familiar with in everyday space. Here's a general formula:\n",
        "\n",
        "$$d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}$$\n",
        "\n",
        "where $p$ and $q$ are two points in the Euclidean space, $n$ is the n-th dimension of the Euclidean space.\n",
        "\n",
        "#### Manhattan distance (Taxicab distance)\n",
        "\n",
        "Manhattan Distance: Also known as city block distance or L1 distance, Manhattan distance calculates the distance between two points by summing the absolute differences of their Cartesian coordinates. It resembles the distance a car would travel along city blocks to reach from one point to another. The general formula is:\n",
        "\n",
        "$$d(\\mathbf{p}, \\mathbf{q}) = \\sum_{i=1}^{n} |p_i-q_i|$$\n",
        "\n",
        "where $p$ and $q$ are two points in the Euclidean space, $n$ is the n-th dimension of the Euclidean space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the following packages required to execute the notebook"
      ],
      "metadata": {
        "id": "yfGbHFlUl9Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet matplotlib numpy pandas scikit-learn scipy"
      ],
      "metadata": {
        "id": "zitPF0eEl9M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries"
      ],
      "metadata": {
        "id": "IwOJaBOUl89O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import tree\n",
        "\n",
        "seed = 520"
      ],
      "metadata": {
        "id": "fg5sA1Jxl84e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV2bG7SFly_f"
      },
      "outputs": [],
      "source": [
        "def euclidean_dist(pA, pB):\n",
        "    \"\"\"\n",
        "    Task: calculate the Euclidean distance between two points\n",
        "\n",
        "    Args:\n",
        "    - pA (list): coordinates of pA in n-th dimension\n",
        "    - pB (list): coordinates of pB in n-th dimension\n",
        "\n",
        "    Return:\n",
        "    - dist (float): Euclidean distance between two points\n",
        "    \"\"\"\n",
        "    # Hint: you might need to use library \"math\" to calculate square root\n",
        "    n_dimension = len(pA)\n",
        "    sum = 0\n",
        "    for i in range(0, n_dimension):\n",
        "        temp = (pA[i] - pB[i]) ** 2\n",
        "        sum += temp\n",
        "\n",
        "    dist = math.sqrt(sum)\n",
        "\n",
        "    return dist\n",
        "\n",
        "def manhattan_dist(pA, pB):\n",
        "    \"\"\"\n",
        "    Task: calculate the Manhattan distance between two points\n",
        "\n",
        "    Args:\n",
        "    - pA (list): coordinates of pA in n-th dimension\n",
        "    - pB (list): coordinates of pB in n-th dimension\n",
        "\n",
        "    Return:\n",
        "    - dist (float): Manhattan distance between two points\n",
        "    \"\"\"\n",
        "    n_dimension = len(pA)\n",
        "    dist = 0\n",
        "    for i in range(0, n_dimension):\n",
        "        temp = abs(pA[i] - pB[i])\n",
        "        dist += temp\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc4Y-VJsly_f"
      },
      "outputs": [],
      "source": [
        "assert euclidean_dist([0, 0], [6, 8]) == 10\n",
        "assert round(euclidean_dist([9, 9], [8, 8]), 3) == 1.414\n",
        "assert round(euclidean_dist([3, 5, 1], [0, 2, 9]), 3) == 9.055\n",
        "\n",
        "assert manhattan_dist([0, 0], [6, 8]) == 14\n",
        "assert manhattan_dist([9, 9], [8, 8]) == 2\n",
        "assert manhattan_dist([3, 5, 1], [0, 2, 9]) == 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJe6UvMFly_f"
      },
      "source": [
        "In K-Nearest Neighbors (KNN), the parameter 𝐾 determines the number of nearest neighbors considered when making predictions. Choosing 𝐾 impacts how the model interprets and predicts based on the data's local structure. A small 𝐾 makes predictions sensitive to the nearest neighbor, while a larger 𝐾 smooths out decisions but risks oversimplifying the data and increasing bias.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "* Requires minimal parameters for training: 𝐾 and the distance metric.\n",
        "* Adapts dynamically to new data points, adjusting predictions as the dataset evolves.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "* Slower processing time with larger 𝐾 due to increased computational effort.\n",
        "* High storage requirements as the model stores the entire dataset, impacting memory usage and execution time.\n",
        "\n",
        "We will create a synthetic dataset from scratch using NumPy. In this exercise, you will create a KNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeUUwDlCly_g"
      },
      "outputs": [],
      "source": [
        "def generate_data(means, cov, N, K):\n",
        "    \"\"\"\n",
        "    Generate synthetic dataset\n",
        "\n",
        "    Args:\n",
        "    - means (list of lists): Containing K mean vectors, each of length d\n",
        "    - cov (list of lists): The covariance matrix of shape (d, d)\n",
        "    - N (int): The number of samples to generate for each cluster\n",
        "    - K (int): The number of clusters\n",
        "\n",
        "    Returns:\n",
        "    - X (np.ndarray): An array of shape (N*K, d) containing the generated data points\n",
        "    - original_label (np.ndarray): An array of shape (N*K,) containing the true labels for the generated data points\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    X0 = np.random.multivariate_normal(means[0], cov, N)\n",
        "    X1 = np.random.multivariate_normal(means[1], cov, N)\n",
        "    X2 = np.random.multivariate_normal(means[2], cov, N)\n",
        "    X = np.concatenate((X0, X1, X2), axis = 0)\n",
        "    original_label = np.asarray([0]*N + [1]*N + [2]*N).T\n",
        "\n",
        "    return X, original_label\n",
        "\n",
        "def kmeans_display(X, label):\n",
        "    \"\"\"\n",
        "    Visualize the dataset\n",
        "\n",
        "    Args:\n",
        "    - X (np.ndarray): An array of shape (N*K, d) containing the generated data points\n",
        "    - label (np.ndarray): An array of shape (N*K,) containing the true labels for the generated data points\n",
        "    \"\"\"\n",
        "    K = np.amax(label) + 1\n",
        "    X0 = X[label == 0, :]\n",
        "    X1 = X[label == 1, :]\n",
        "    X2 = X[label == 2, :]\n",
        "\n",
        "    plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\n",
        "    plt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\n",
        "    plt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)\n",
        "\n",
        "    plt.axis('equal')\n",
        "    plt.plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K5k-yIkly_g"
      },
      "outputs": [],
      "source": [
        "N, K = 500, 3\n",
        "means = [[2, 2], [7, 3], [3, 6]]\n",
        "cov = [[1, 0], [0, 1]]\n",
        "\n",
        "# Generate and visualize data\n",
        "X, original_label = generate_data(means, cov, N, K)\n",
        "kmeans_display(X, original_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dk-FziHly_g"
      },
      "outputs": [],
      "source": [
        "def kmeans_init_centers(X, nc):\n",
        "    \"\"\"\n",
        "    Initialize the centers\n",
        "\n",
        "    Args:\n",
        "    - X (np.ndarray): An array of shape (N*K, d) containing the generated data points\n",
        "    - K (int): number of centers to be intialized\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: An array of shape (nc, d) containing the initialized centers.\n",
        "    \"\"\"\n",
        "    # randomly pick k rows of X as initial centers using np.random.choice function\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    row = np.random.choice(X.shape[0], nc, replace=False)\n",
        "    return X[row]\n",
        "\n",
        "def kmeans_assign_labels(X, centers):\n",
        "    \"\"\"\n",
        "    Assign labels to each data point based on the nearest center.\n",
        "\n",
        "    Args:\n",
        "    - X (np.ndarray): An array of shape (N*K, d) containing the generated data points\n",
        "    - centers (np.ndarray): An array of shape (k, d) containing the current center coordinates.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: An array of shape (N*K,) containing the index of the closest center for each data point.\n",
        "    \"\"\"\n",
        "\n",
        "    pair_distance = cdist(X, centers)\n",
        "    return np.argmin(pair_distance, axis=1)\n",
        "\n",
        "\n",
        "def kmeans_update_centers(X, labels, K):\n",
        "    \"\"\"\n",
        "    Update the center coordinates based on the assigned labels.\n",
        "\n",
        "    Args:\n",
        "    - X (np.ndarray): An array of shape (N*K, d) containing the generated data points\n",
        "    - labels (np.ndarray): An array of shape (N*K,) containing the labels for each data point.\n",
        "    - K (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: An array of shape (K, d) containing the updated center coordinates.\n",
        "    \"\"\"\n",
        "    centers = np.zeros((K, X.shape[1]))\n",
        "    for i in range(K):\n",
        "        # collect all points assigned to the i-th cluster\n",
        "        # take average\n",
        "\n",
        "        index = np.where(labels == i)[0]\n",
        "        centers[i, :] = np.mean(X[index], axis=0)\n",
        "\n",
        "    return centers\n",
        "\n",
        "def has_converged(centers, new_centers):\n",
        "    \"\"\"\n",
        "    Check if the centers have converged.\n",
        "\n",
        "    Args:\n",
        "    - centers (np.ndarray): An array of shape (k, d) containing the old center coordinates.\n",
        "    - new_centers (np.ndarray): An array of shape (k, d) containing the new center coordinates.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the centers have converged, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    og_set = set([tuple(center) for center in centers])\n",
        "    new_set = set([tuple(center) for center in new_centers])\n",
        "    return og_set == new_set\n",
        "\n",
        "def kmeans(X, K):\n",
        "    \"\"\"\n",
        "    Perform K-means clustering.\n",
        "\n",
        "    Args:\n",
        "    - X (np.ndarray): An array of shape (N*K, d) containing the generated data points\n",
        "    - K (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing:\n",
        "        + centers (list of np.ndarray): A list containing the center coordinates at each iteration.\n",
        "        + labels (list of np.ndarray): A list containing the labels at each iteration.\n",
        "        + it (int): The number of iterations until convergence.\n",
        "    \"\"\"\n",
        "    # save the center coordinates of each iteration\n",
        "    centers = [kmeans_init_centers(X, K)]\n",
        "    # save the labels of each iteration\n",
        "    labels = []\n",
        "    it = 0\n",
        "    while True:\n",
        "        # at each iteration:\n",
        "        # 1. assign label for each points and append to labels\n",
        "        # 2. update the centers\n",
        "        # 3. check the convergence condition\n",
        "        #    and append NEW center coordinates to centers\n",
        "        # 4. update iteration\n",
        "\n",
        "        labels.append(kmeans_assign_labels(X, centers[-1]))\n",
        "        new_centers = kmeans_update_centers(X, labels[-1], K)\n",
        "        if has_converged(centers[-1], new_centers):\n",
        "            break\n",
        "        centers.append(new_centers)\n",
        "        it += 1\n",
        "\n",
        "    return (centers, labels, it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291r81b4ly_g"
      },
      "outputs": [],
      "source": [
        "K = 3\n",
        "(centers, labels, it) = kmeans(X, K)\n",
        "kmeans_display(X, labels[-1])\n",
        "\n",
        "assert centers[-1].shape == (3, 2)\n",
        "\n",
        "centers = kmeans_init_centers(X, K)\n",
        "assert centers.shape == (K, 2)\n",
        "\n",
        "assigned_labels = kmeans_assign_labels(X, centers)\n",
        "assert assigned_labels.shape == (N*3, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXpTyE1yly_h"
      },
      "source": [
        "We'll be using `sklearn` to create a KNN model to classify types of flowers.\n",
        "\n",
        "Using the sklearn library to perform each step:\n",
        "- Step 1: Split the dataset into 2 sets: a train set and a test set (70% train, 30% test)\n",
        "- Step 2: Create a KNN model with 5 buckets/clusters\n",
        "- Step 3: Test how accurate the model is with test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2sTWfvzly_h"
      },
      "outputs": [],
      "source": [
        "iris = pd.read_csv(\"https://raw.githubusercontent.com/chenethanxd/data/main/iris.csv\")\n",
        "X = iris.drop('target', axis=1)\n",
        "y = iris['target']\n",
        "\n",
        "# Step 1\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
        "# Step 2\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_X, train_y)\n",
        "# Step 3\n",
        "pred_y = knn.predict(test_X)\n",
        "accuracy = metrics.accuracy_score(pred_y,test_y)\n",
        "print('The accuracy of the KNN is', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41TVJEjMly_h"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Logistic regression is a binary classification method that utilizes the sigmoid function to predict probabilities between 0 and 1 for each input. In this method, inputs are classified into two classes—Class 0 and Class 1—based on whether the sigmoid output exceeds a threshold (typically 0.5). Despite its name, logistic regression extends from linear regression and is primarily employed for classification tasks. It estimates the probability of an input belonging to a particular class based on its explanatory variables, making it a versatile tool in predictive modeling.\n",
        "\n",
        "### Sigmoid function\n",
        "\n",
        "The sigmoid function is an activation function commonly used in machine learning and neural networks. It takes an input value and outputs a probability score between 0 and 1. Here's the formula:\n",
        "\n",
        "$$\\sigma(z)=\\dfrac{1}{1+e^{-z}}$$\n",
        "\n",
        "where $z$ is the weighted sum of the model function\n",
        "\n",
        "The value from Sigmoid function always lies between 0 and 1, with 0.5 being the threshold for determining the binary label from the features.\n",
        "\n",
        "### Hypothesis function\n",
        "\n",
        "Hypothesis function is a function used to predict the output value based on input features and using learned weights and a bias. Here's the formula:\n",
        "\n",
        "$$H=\\sigma(w^TX+b)$$\n",
        "\n",
        "where $w^T$ is the transpose of the weight matrix, $X$ is the input matrix, and $b$ is the bias matrix.\n",
        "\n",
        "### Cost Function\n",
        "\n",
        "A cost function quantifies the error or discrepancy between predicted and actual values in a machine learning model, guiding optimization towards minimizing this error. Here's the formula:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]$$\n",
        "\n",
        "where $m$ is the number of training examples, $y^{(i)}$ is the true label for the i-th training example, $x^{(i)}$ is the feature vector for the i-th training example, $\\theta$ is the vector of parameters (weights), and $h_\\theta$ is the predicted probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chv1JpHxly_h"
      },
      "source": [
        "We'll be using Sklearn to perform Logistic Regression. Since it's using binary classification, we'll use a different dataset to detect diabetes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FuBPB4-ly_h"
      },
      "outputs": [],
      "source": [
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z.\n",
        "\n",
        "    Parameters:\n",
        "    z (numpy.ndarray): Input array.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Sigmoid of the input.\n",
        "    \"\"\"\n",
        "    result = 1. / (1 + np.exp(-z))\n",
        "    return result\n",
        "\n",
        "# Predict function using the sigmoid\n",
        "def predict(x, weights):\n",
        "    \"\"\"\n",
        "    Predict the probability using logistic regression model.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy.ndarray): Feature matrix with shape (n_samples, n_features).\n",
        "    weights (numpy.ndarray): Weights array with shape (n_features,).\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Predicted probabilities.\n",
        "    \"\"\"\n",
        "    result = sigmoid(np.dot(x, weights))\n",
        "    return result\n",
        "\n",
        "def compute_cost(weights, x, y):\n",
        "    \"\"\"\n",
        "    Compute the binary cross-entropy cost.\n",
        "\n",
        "    Parameters:\n",
        "    weights (numpy.ndarray): Weights array with shape (n_features,).\n",
        "    x (numpy.ndarray): Feature matrix with shape (n_samples, n_features).\n",
        "    y (numpy.ndarray): True labels with shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "    float: The binary cross-entropy cost.\n",
        "    \"\"\"\n",
        "    h = predict(x, weights)\n",
        "    one_case = np.dot(-y, np.log(h))\n",
        "    zero_case = np.dot((1 - y), np.log(1 - h))\n",
        "    return (one_case + zero_case) / len(y)\n",
        "\n",
        "# Gradient descent function to update weights\n",
        "def gradient_descent(weights, x, y, learning_rate):\n",
        "    \"\"\"\n",
        "    Perform one step of gradient descent on the weights.\n",
        "\n",
        "    Parameters:\n",
        "    weights (numpy.ndarray): Current weights with shape (n_features,).\n",
        "    x (numpy.ndarray): Feature matrix with shape (n_samples, n_features).\n",
        "    y (numpy.ndarray): True labels with shape (n_samples,).\n",
        "    learning_rate (float): Learning rate for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Updated weights.\n",
        "    \"\"\"\n",
        "    error = predict(x, weights) - y\n",
        "    gradient = np.dot(x.T, error) / len(x)\n",
        "    weights -= learning_rate * gradient\n",
        "    return weights\n",
        "\n",
        "# Minimize function to train the model\n",
        "def minimize(weights, x, y, iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    Minimize the cost function using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    weights (numpy.ndarray): Initial weights with shape (n_features,).\n",
        "    x (numpy.ndarray): Feature matrix with shape (n_samples, n_features).\n",
        "    y (numpy.ndarray): True labels with shape (n_samples,).\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "    learning_rate (float): Learning rate for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Updated weights and list of costs over iterations.\n",
        "    \"\"\"\n",
        "    costs = []\n",
        "    for i in range(iterations):\n",
        "        weights = gradient_descent(weights, x, y, learning_rate)\n",
        "        cost = compute_cost(weights, x, y)\n",
        "        costs.append(cost)\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch {i}, Loss: {cost}')\n",
        "    return weights, costs\n",
        "\n",
        "# Evaluate function to calculate accuracy\n",
        "def evaluate(X, y, weights):\n",
        "    \"\"\"\n",
        "    Evaluate the logistic regression model using accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix with shape (n_samples, n_features).\n",
        "    y (numpy.ndarray): True labels with shape (n_samples,).\n",
        "    weights (numpy.ndarray): Weights array with shape (n_features,).\n",
        "\n",
        "    Returns:\n",
        "    float: Accuracy of the model.\n",
        "    \"\"\"\n",
        "    y_pred = predict(X, weights) >= 0.5\n",
        "    accuracy = np.mean(y_pred == y)\n",
        "    return accuracy\n",
        "\n",
        "# Main code\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "diabetes = pd.read_csv(\"https://raw.githubusercontent.com/chenethanxd/data/main/diabetes.csv\")\n",
        "X = diabetes.drop('target', axis=1)\n",
        "y = diabetes.target\n",
        "\n",
        "# Convert target to binary\n",
        "y = (y >= np.median(y)).astype(int)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Add intercept term\n",
        "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
        "train_X = scaler.fit_transform(train_X)\n",
        "test_X = scaler.transform(test_X)\n",
        "\n",
        "# Initialize weights\n",
        "np.random.seed(seed)\n",
        "weights = np.random.randn(train_X.shape[1])\n",
        "\n",
        "# Train the model\n",
        "weights, costs = minimize(weights, train_X, train_y, epochs, learning_rate)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = evaluate(train_X, train_y, weights)\n",
        "test_accuracy = evaluate(test_X, test_y, weights)\n",
        "\n",
        "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Step 1: Split the dataset into 2 sets: a train set and a test set (70% train, 30% test, random_state=seed)\n",
        "- Step 2: Standardize the train_X and test_X\n",
        "- Step 3: Create a Logisctic Regression model with default parameters\n",
        "- Step 4: Test how accurate the model is with test set"
      ],
      "metadata": {
        "id": "pVSgtW83fidH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjoki9RNly_h"
      },
      "outputs": [],
      "source": [
        "X = diabetes.drop('target', axis=1)\n",
        "y = diabetes.target\n",
        "y = (y >= np.median(y)).astype(int)\n",
        "\n",
        "# Step 1\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
        "# Step 2\n",
        "scaler = StandardScaler()\n",
        "train_X = scaler.fit_transform(train_X)\n",
        "test_X = scaler.transform(test_X)\n",
        "# Step 3\n",
        "log_reg = LogisticRegression(penalty=None)\n",
        "log_reg.fit(train_X, train_y)\n",
        "# # Step 4\n",
        "# log_reg.score(test_X, test_y)  #Use score or .predict, followed by accuracy_score\n",
        "pred_y = log_reg.predict(test_X)\n",
        "accuracy = metrics.accuracy_score(pred_y,test_y)\n",
        "print(f'The accuracy of the Logistic Regression is {round(accuracy * 100, 2)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5A7nipzly_h"
      },
      "source": [
        "## Support Vector Machine (SVM)\n",
        "\n",
        "In machine learning, support vector machines (SVMs) are models used for supervised learning tasks like classification and regression. SVM algorithms analyze data by creating a model from a set of training examples, each labeled as belonging to one of two categories. This model then categorizes new examples into one of these categories, effectively creating a binary linear classifier that is nonprobabilistic in nature.\n",
        "\n",
        "In practical applications, SVMs are advantageous for handling large datasets where accurately determining the correct class for millions of training examples can be computationally intensive.\n",
        "\n",
        "For instance, imagine using SVMs to predict whether someone enjoys a film based on how much they consumed in a cinema. Initially, a logistic regression model might be used for classification. However, this approach may lead to biased deductions, such as assuming that people who consume a lot of popcorn enjoy the film, and vice versa. SVMs address this issue by transforming the data, often by adding a new dimension (axis), which allows for the creation of a clear boundary (hyperplane) that separates different classes, enhancing the model's classification accuracy.\n",
        "\n",
        "### Kernel\n",
        "\n",
        "The process of learning the hyperplane in support vector machines (SVMs) involves transforming the problem using techniques from linear algebra.\n",
        "\n",
        "### Regularization\n",
        "\n",
        "The Regularization parameter (C) in SVM controls the trade-off between achieving a low training error and a low testing error, which is achieved by maximizing the margin. For large values of C, the optimization will prioritize a smaller-margin hyperplane that classifies all training points correctly. Conversely, a small value of C will result in a larger-margin hyperplane that may misclassify more points but aims to generalize better by focusing less on the individual points.\n",
        "\n",
        "### Gamma\n",
        "\n",
        "The gamma parameter defines how far the influence of a single training example reaches. Low $\\gamma$ values mean that the influence reaches far, considering points far away from the separation line in the calculation. High $\\gamma$ values mean the influence is close, considering only points near the separation line in the calculation.\n",
        "\n",
        "### Margin\n",
        "\n",
        "A margin is a separation of line to the closest class points.\n",
        "\n",
        "A good margin is one where this separation is larger for both the classes. A good margin allows the points to be in their respective classes without crossing to other class. However, a bad margin will be close or biased towards either classes.\n",
        "\n",
        "### Types of SVM\n",
        "\n",
        "* Linear SVM: This is used when the data is perfectly linearly separable, meaning the data points can be classified into two classes using a single straight line.\n",
        "* Non-linear SVM: This is used when the data is not linearly separable. In such cases, advanced techniques like kernel tricks are used to classify the data points into two classes that cannot be separated by a straight line.\n",
        "\n",
        "Here's general pseudocode of how SVM executes:\n",
        "```\n",
        "1. Input: Training data (X, y), where X is the set of features and y is the set of labels.\n",
        "2. Initialize the weights (w) and bias (b) to small random values.\n",
        "3. Set the learning rate (α) and the regularization parameter (λ).\n",
        "4. Repeat until convergence or for a fixed number of iterations:\n",
        "    a. For each training example (x_i, y_i):\n",
        "        1. Compute the prediction: y_pred = w * x_i + b\n",
        "        2. If y_i * y_pred < 1 (misclassification or within margin):\n",
        "            - Update the weights: w = w - α * (λ * w - y_i * x_i)\n",
        "            - Update the bias: b = b - α * (-y_i)\n",
        "        3. Else (correct classification):\n",
        "            - Update the weights: w = w - α * (λ * w)\n",
        "5. The resulting w and b define the hyperplane.\n",
        "6. To make predictions on new data points x:\n",
        "    a. Compute the prediction: y_pred = w * x + b\n",
        "    b. Assign the class based on the sign of y_pred\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ-8_bcely_i"
      },
      "source": [
        "We'll be using SVC to perform Support Vector Machine Classification. We'll use a different dataset to detect cancer.\n",
        "\n",
        "Using the sklearn library to perform each step:\n",
        "- Step 1: Split the dataset into 2 sets: a train set and a test set (70% train, 30% test, random_state=seed)\n",
        "- Step 2: Create an SVM model with kernel is \"rbf\", gamma is 0.5 and C is 1.0. Assign this to variable `svm`\n",
        "- Step 3: Test how accurate the model is with test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8RqU-5uly_i"
      },
      "outputs": [],
      "source": [
        "cancer = pd.read_csv(\"https://raw.githubusercontent.com/chenethanxd/data/main/cancer.csv\")\n",
        "X = cancer.iloc[:, :2]\n",
        "y = cancer.target\n",
        "\n",
        "# Step 1\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
        "# Step 2\n",
        "svm = SVC(kernel=\"rbf\", gamma=0.5, C=1.0)\n",
        "svm.fit(train_X, train_y)\n",
        "# Step 3\n",
        "accuracy = svm.score(test_X, test_y)\n",
        "#can also use .predict followed by accuracy metric function\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaZKK1-uly_i"
      },
      "outputs": [],
      "source": [
        "DecisionBoundaryDisplay.from_estimator(\n",
        "    svm,\n",
        "    X,\n",
        "    response_method=\"predict\",\n",
        "    cmap=plt.cm.Spectral,\n",
        "    alpha=0.8,\n",
        "    )\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(\n",
        "    X.iloc[:, 0],\n",
        "    X.iloc[:, 1],\n",
        "    c=y,s=20,\n",
        "    edgecolors=\"k\"\n",
        "    )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiVlA-7Nly_i"
      },
      "source": [
        "## Decision Tree\n",
        "\n",
        "A decision tree is a non-parametric supervised learning algorithm with a hierarchical, tree structure consisting of a root node, branches, internal nodes, and leaf nodes.\n",
        "\n",
        "The logic behind a decision tree mimics human decision-making: if A happens, then B is the result; if A doesn't happen, then something different from B is the result. Essentially, it's an if-else tree structure.\n",
        "\n",
        "In a decision tree, the final result comes from the leaf nodes, while the best variable to start predicting with is the root node.\n",
        "\n",
        "### Entropy\n",
        "\n",
        "Entropy is a metric used to measure the impurity or randomness in a given attribute. It quantifies the unpredictability in the data. Higher entropy indicates more disorder and more information content, meaning the data is more mixed or less uniform. Lower entropy suggests uniformity or consistency in the information content across all data points. The formula is:\n",
        "\n",
        "$$H(X)=-\\sum^n_{i=1}p_i \\log_2 p_i$$\n",
        "\n",
        "where $X$ is the total number of samples, $p_i$ is the probability of class p. If the result is 0, the label value from all rows are of same type. If it's 1, the label values from all rows are distributed equally. For instance, if there are 10 dogs in the label column, the result will be 0. On the other hand, 5 dogs and 5 cats will result in a 1.\n",
        "\n",
        "### Gini Index\n",
        "\n",
        "The Gini index is a metric used to measure the impurity or purity of a dataset while creating a decision tree. It quantifies the likelihood of a random sample being incorrectly classified if it were randomly labeled according to the distribution of labels in the dataset. A Gini index of 0 indicates perfect purity, where all elements belong to a single class, while higher values indicate greater impurity. Here's the formula:\n",
        "\n",
        "$$Gini=1-\\sum^n_{i=1}p_i^2$$\n",
        "\n",
        "### Decision Tree Components\n",
        "\n",
        "* Root Node: The top node that represents the entire dataset and is the starting point for the tree's splits.\n",
        "* Decision Nodes: Intermediate nodes where the data gets split based on specific features.\n",
        "* Leaf Nodes: Terminal nodes that represent the final class labels or regression values.\n",
        "* Branches: The paths that connect decision nodes to leaf nodes or other decision nodes, representing the flow of the dataset through the tree.\n",
        "\n",
        "1. Select the best attribute to split the records. Initially, we consider all attributes to be root node.\n",
        "2. Calculate the Entropy of the label column $H(label)$\n",
        "3. For each feature of an attribute column, we calculate the $H(feature)$. For instance, if there are 2 features: 12 rain rows and 15 sunny rows, we will calculate $H(rain)$ of 12 rows and its corresponding label and so on.\n",
        "4. Calculate the weighted entropy of that attribute column. The general formula is: $\\text{Weighted H(attr)}=\\sum^{|features|}_{i=1}\\dfrac{\\text{Total num. of rows with feature}_i}{\\text{Total rows}}*H(i)$. The weighted value can also be represented as a conditional probability $H(attr|label)$\n",
        "5. We calculate the $InfoGain (=H(attr) - H(attr|label))$ of each attribute and choose the highest one to be the root.\n",
        "6. Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
        "7. Start building tree by repeating this process recursively for each child until one of the conditions matches below:\n",
        "    * All the tuples belong to the same attribute value.\n",
        "    * There are no more remaining attributes.\n",
        "    * There are no more instances.\n",
        "\n",
        "An example decision tree after choosing the root node and recursively building the decision nodes and branches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5lb3EVHly_i"
      },
      "source": [
        "We'll be using Sklearn to create a Decision Tree. We will be using the Iris dataset to classify based on different features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQg7HkPfly_i"
      },
      "outputs": [],
      "source": [
        "iris = pd.read_csv(\"https://raw.githubusercontent.com/chenethanxd/data/main/iris.csv\")\n",
        "X = iris.iloc[:, :-1]\n",
        "y = iris.target\n",
        "\"\"\"\n",
        "Using sklearn library to perform each step\n",
        "Step 1: Split the dataset into 2 sets: a train set and a test set (70% train, 30% test)\n",
        "Step 2: Create a Decision Tree with Gini Index as criteria to classify\n",
        "Step 3: Test how accurate the model is with test set\n",
        "\"\"\"\n",
        "# Step 1\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
        "# Step 2\n",
        "gini = DecisionTreeClassifier(criterion='gini')\n",
        "gini.fit(train_X, train_y)\n",
        "# Step 3\n",
        "pred_y = gini.predict(test_X)\n",
        "#todo: replace metrics.accuracy_score(pred_y, test_y) with gini.score(test_X, test_y)\n",
        "\n",
        "accuracy = metrics.accuracy_score(pred_y,test_y)\n",
        "print('The accuracy of the Decision Tree using Gini Index is', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfhbzdQWly_i"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "tree.plot_tree(gini.fit(train_X, train_y),\n",
        "               feature_names=iris.columns[:-1],\n",
        "               class_names=[\"setosa\", \"versicolor\", \"virginica\"],\n",
        "               rounded=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LG2FdGDly_i"
      },
      "source": [
        "## Evaluation metrics\n",
        "\n",
        "### Confusion matrix\n",
        "\n",
        "A confusion matrix is a table that summarizes the classification results of a binary classifier. It provides a detailed analysis of a model's performance by showing the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The definitions of these four components are as follows:\n",
        "\n",
        "* True Positive (TP): The number of positive instances correctly classified as positive by the model.\n",
        "* False Positive (FP): The number of negative instances incorrectly classified as positive by the model.\n",
        "* True Negative (TN): The number of negative instances correctly classified as negative by the model.\n",
        "* False Negative (FN): The number of positive instances incorrectly classified as negative by the model.\n",
        "\n",
        "### Accuracy\n",
        "\n",
        "Accuracy is the most commonly used performance metric for evaluating a binary classification model. It measures the proportion of correct predictions made by the model out of all the predictions. A high accuracy score indicates that the model is making a large proportion of correct predictions whereas a low score indicates the model is making too many incorrect predictions. Accuracy is calculated using the following formula:\n",
        "\n",
        "$$\\text{Accuracy}=\\dfrac{TP+TN}{TP+TN+FP+FN}$$\n",
        "\n",
        "### Precision\n",
        "\n",
        "Precision is a metric that measures the accuracy of positive predictions made by the model. A high precision indicates that the model is making a low number of false positive predictions whereas a low score indicates tha the model is making too many. Precision is calculated using the following formula:\n",
        "\n",
        "$$\\text{Precision}=\\dfrac{TP}{TP+FP}$$\n",
        "\n",
        "### Recall\n",
        "\n",
        "Recall, also known as sensitivity or true positive rate (TPR), measures the proportion of TPs among all the actual positive instances. In other words, recall measures the model’s ability to correctly identify positive instances. A high recall score indicates that the model is able to identify a large proportion of positive instances, while a low recall score indicates that the model is missing many positive instances. Recall is an important metric in evaluating the performance of a binary classification model. It measures the model’s ability to correctly identify positive instances. Recall is calculated using the following formula:\n",
        "\n",
        "$$\\text{Recall}=\\dfrac{TP}{TP+FN}$$\n",
        "\n",
        "### F1-score\n",
        "\n",
        "The F1-score is a metric that combines precision and recall into a single measure to evaluate the overall performance of a binary classification model. It calculates the harmonic mean of precision and recall, providing a balanced assessment of both metrics. A high score indicates that the model has both high precision and high recall, reflecting strong performance in correctly identifying positive instances and minimizing false positives.A low F1-score indicates that the model is either lacking in precision (making too many false positives) or recall (missing many positive instances). F1-score is particularly useful when you want to balance the trade-off between precision and recall, especially in scenarios where both metrics are equally important. F1-score is calculated using the following formula:\n",
        "\n",
        "$$\\text{F1-score}=\\dfrac{2 * precision * recall}{precision + recall}$$\n",
        "\n",
        "### AUC-ROC curve\n",
        "\n",
        "One of the most widely used performance metrics is the AUC-ROC curve. AUC stands for Area Under the Curve, and ROC stands for Receiver Operating Characteristic. The ROC curve is a graphical representation of the performance of a binary classifier, indicating the tradeoff between true positive rate (TPR) and false positive rate (FPR) at different thresholds. The AUC-ROC curve is the plot of the TPR against FPR at various threshold settings. The AUC represents the area under this curve, which ranges from 0 to 1, with a higher AUC indicating better model performance. The TPR and FPR are defined as follows:\n",
        "\n",
        "$$TPR = \\dfrac{TP}{TP + FN}$$\n",
        "\n",
        "$$FPR = \\dfrac{FP}{FP+TN}$$\n",
        "\n",
        "The ROC curve is obtained by plotting TPR against FPR at different thresholds. A perfect classifier would have a ROC curve that passes through the top-left corner (TPR = 1 and FPR = 0), while a random classifier would have a ROC curve that passes through the diagonal (TPR = FPR)\n",
        "\n",
        "Usually we don't need to implement these from scratch since we cannot *explicitly* know every answer for every result. Instead, we'll be using existing packages to help us do the work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFrcpQKsly_j"
      },
      "outputs": [],
      "source": [
        "y_true = [0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "y_pred = [0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1]\n",
        "\n",
        "# Please calculate the accuracy, precision, recall and f1-score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(accuracy, precision, recall, f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TBr6gp8ly_j"
      },
      "outputs": [],
      "source": [
        "assert round(accuracy, 2) == 0.75\n",
        "assert round(precision, 2) == 0.7\n",
        "assert round(recall, 2) == 0.78\n",
        "assert round(f1, 2) == 0.74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEiPoSFily_j"
      },
      "outputs": [],
      "source": [
        "# True labels\n",
        "y_true = [0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# Predicted probabilities (for ROC curve, we need probabilities rather than binary predictions)\n",
        "# For the purpose of this example, we will create some example probabilities\n",
        "# In a real scenario, these probabilities would be obtained from the model's predict_proba method\n",
        "y_pred_prob = [0.1, 0.9, 0.2, 0.3, 0.8, 0.6, 0.4, 0.3, 0.7, 0.8, 0.2, 0.5, 0.9, 0.4, 0.3, 0.2, 0.8, 0.1, 0.7, 0.6]\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cw5VapKly_j"
      },
      "source": [
        "## References\n",
        "\n",
        "+ [Evaluation metrics in Classification](https://medium.com/@impythonprogrammer/evaluation-metrics-for-classification-fc770511052d)\n",
        "+ [Decision Tree](https://medium.com/intro-to-artificial-intelligence/decision-tree-learning-e153b5b4ecdf)\n",
        "+ [SVM pseudo code](https://www.kaggle.com/code/prashant111/svm-classifier-tutorial)\n",
        "+ [SVM theory](https://medium.com/@AnasBrital98/introduction-to-support-vector-machine-2a2091401858)\n",
        "+ [Logistic Regression](https://github.com/TrishlaM/Logistic-regression-from-scratch-using-NumPy/blob/master/Logistic_regression_from_scratch.ipynb)\n",
        "+ [Logistic Regression from scratch](https://towardsdatascience.com/logistic-regression-from-scratch-with-numpy-da4cc3121ece)\n",
        "+ [KNN](https://www.geeksforgeeks.org/k-nearest-neighbours/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{"cells":[{"cell_type":"markdown","id":"183ade39-0a8b-413c-970f-23ebca36532a","metadata":{"id":"183ade39-0a8b-413c-970f-23ebca36532a"},"source":["# KMeans"]},{"cell_type":"markdown","id":"260c5411-8db0-4d2f-bb9a-7bf2b12e8bac","metadata":{"id":"260c5411-8db0-4d2f-bb9a-7bf2b12e8bac"},"source":["## What is KMeans?\n","\n","- KMeans is an unsupervised machine learning algorithm used for clustering data into **'K'** groups based on similarities.\n","- It **iteratively** refines the **clusters** by **adjusting the positions of cluster centroids** and reassigning data points to the nearest centroid until convergence.\n","- K-means is one of the most widely used clustering algorithms in machine learning.\n","- Unsupervised learning refers to learning without using pre-existing labels or categories (meaning the algorithm must discover patterns or groups within the data without prior training).\n","\n","## How does KMeans work?\n","\n","The way K-means clustering works is as follows:\n","- We start with a dataset of items, each with certain features and corresponding values.\n","- First, we randomly initialize **k** points (known as means or cluster centroids).\n","- Next, we assign each item to its closest mean and then update the mean's coordinates based on the averages of the items assigned to that cluster.\n","- This process is repeated for a specified number of iterations to determine the final clusters.\n","\n","## Choosing the Number of Clusters\n","- The optimal number of clusters, **'k'**, for the algorithm is typically determined using methods like: **Elbow method** and  **Silhouette score**.\n","\n","### Elbow Method\n","- The Elbow Method involves calculating the `within-cluster sum of squares (WCSS)` for a range of 'k' values and plotting these against **'k'**.\n","- **WCSS** is a measure of the total variance within each cluster. It quantifies how spread out the points within a cluster are from the cluster centroid.\n","- The within-cluster sum of squares (WCSS) is calculated as follows:\n","\n","$$ \\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 $$\n","    \n","where:\n","- **k** is the number of clusters.\n","- **Ci** represents the i-th cluster.\n","- **x** is a data point in cluster **Ci**.\n","- **μi** is the centroid (mean) of cluster **Ci**.\n","- **|x - μi|^2** is the squared Euclidean distance between a data point **x** and its cluster centroid **μi**.\n","\n","**Steps:**\n","1. Calculate WCSS for different values of **k**\n","    - Run the KMeans algorithm for a range of cluster numbers (e.g., k=1 to k=10).\n","    - For each k, compute the WCSS.\n","2. Plot **WCSS** against k:\n","    - Create a plot where the x-axis represents the number of clusters k and the y-axis represents the WCSS.\n","    - As k increases, the WCSS decreases because clusters become smaller and tighter.\n","3. Identify the **'elbow'** point:\n","    - The 'elbow' point on the plot is where the rate of decrease of WCSS slows down. This point indicates the optimal number of clusters.\n","    - Adding more clusters beyond this point does not significantly reduce the WCSS.\n","    - The \"elbow\" point, where the rate of decrease in WCSS slows down, is usually considered the optimal number of clusters.\n","\n","\n","### Silhouette score\n","- The silhouette score measures the quality of clustering by considering both the cohesion within clusters and the separation between clusters.\n","- The silhouette score `s` for a single data point `i` is calculated as follows:\n","\n","$$s = (b - a)/max(a,b)$$\n","\n","where:\n","- **a** represents the average distance from the data point to all other points in the same cluster\n","- **b** represents the minimum average distance from the data point to all points in the next nearest cluster\n","- The score ranges from -1 to 1, where a higher value indicates better-defined clusters.\n","\n","**Steps:**\n","1. Silhouette scores for all points are calculated for a range of values for 'k'.\n","2. The value of 'k' with the highest average silhouette score is generally considered the optimal number of clusters.\n","\n","From the scikit-learn library, the following site outlines the usage and parameters of the KMeans function.\n","https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#"]},{"cell_type":"markdown","id":"084a449a","metadata":{"id":"084a449a"},"source":["**Objectives:**\n","- Simple K-Means Clustering Using Manual Data Points.\n","- K-mean on Synethetic data sample (using `make_blobs`) and Finding Relevant Statistics\n","- K-means Clustering on a Real-World Dataset.\n","- Practical Coding Example"]},{"cell_type":"markdown","id":"71c22887","metadata":{"id":"71c22887"},"source":["## Install the necessary packages."]},{"cell_type":"code","execution_count":null,"id":"3e954e19-a2f6-4cea-a17a-1c43ea35f8d0","metadata":{"id":"3e954e19-a2f6-4cea-a17a-1c43ea35f8d0"},"outputs":[],"source":["%pip install numpy==1.26.0 pandas==2.2.2 scipy==1.10.1 jedi"]},{"cell_type":"code","source":["%pip install --upgrade --quiet scikit-learn matplotlib seaborn kneed"],"metadata":{"id":"TcEZAMo4uNVJ"},"id":"TcEZAMo4uNVJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Authenticate your notebook environment (Colab only)\n","If you are running this notebook on Google Colab, run the cell below to authenticate your environment."],"metadata":{"id":"mO1KC8iLm4eR"},"id":"mO1KC8iLm4eR"},{"cell_type":"code","source":["import sys\n","\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","\n","    auth.authenticate_user()"],"metadata":{"id":"afq8u6awm6ha"},"id":"afq8u6awm6ha","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import the necessary libraries"],"metadata":{"id":"302uYe6enLwU"},"id":"302uYe6enLwU"},{"cell_type":"code","execution_count":null,"id":"3378359a","metadata":{"id":"3378359a"},"outputs":[],"source":["import tarfile\n","import urllib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from kneed import KneeLocator\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, adjusted_rand_score\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline\n","\n","import pandas as pd\n","import seaborn as sns"]},{"cell_type":"markdown","id":"f0e3f08a","metadata":{"id":"f0e3f08a"},"source":["## Simple K-Means Clustering Using Manual Data Points"]},{"cell_type":"markdown","id":"aea19a1c","metadata":{"id":"aea19a1c"},"source":["- First lets plot and visualize manual data points."]},{"cell_type":"code","execution_count":null,"id":"5eb05bbb","metadata":{"id":"5eb05bbb"},"outputs":[],"source":["# Visualize manual data points\n","x = [4, 5, 10, 4, 3, 11, 14, 6, 10, 12]\n","y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n","\n","plt.scatter(x, y)\n","plt.show()"]},{"cell_type":"markdown","id":"857a452f","metadata":{"id":"857a452f"},"source":["- Next we calculate Inertia for Different Values of K, and visualize the result."]},{"cell_type":"code","execution_count":null,"id":"9a12c4be","metadata":{"id":"9a12c4be"},"outputs":[],"source":["data = list(zip(x, y))\n","inertias = []\n","\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters=i)\n","    kmeans.fit(data)\n","    inertias.append(kmeans.inertia_)\n","\n","# Now we utilize the elbow method (discussed more later) to visualize the inertia for different values of K\n","plt.plot(range(1, 11), inertias, marker=\"o\")\n","plt.xlabel(\"Number of clusters\")\n","plt.ylabel(\"Inertia\")\n","plt.show()"]},{"cell_type":"markdown","id":"5ff39c75","metadata":{"id":"5ff39c75"},"source":["- From previous plot, we see that 2 is a good value for k, so we retrain and visualize clusters."]},{"cell_type":"code","execution_count":null,"id":"befbefb5","metadata":{"id":"befbefb5"},"outputs":[],"source":["kmeans = KMeans(n_clusters=2)\n","kmeans.fit(data)\n","\n","plt.scatter(x, y, c=kmeans.labels_)\n","plt.show()"]},{"cell_type":"markdown","id":"3058cc32","metadata":{"id":"3058cc32"},"source":["## K-means on Synthetic data sample (using `make_blobs`) and Finding Relevant Statistics"]},{"cell_type":"markdown","id":"c139fd0b","metadata":{"id":"c139fd0b"},"source":["- Using make_blobs for Synthetic Data"]},{"cell_type":"code","execution_count":null,"id":"f3502abf","metadata":{"id":"f3502abf"},"outputs":[],"source":["features, true_labels = make_blobs(\n","    n_samples=500, n_features=2, centers=3, random_state=23\n",")\n","\n","fig = plt.figure(0)\n","plt.grid(True)\n","plt.scatter(features[:, 0], features[:, 1])\n","plt.show()"]},{"cell_type":"markdown","id":"ac0589f8-a6a5-4a91-989d-71f222e6502d","metadata":{"id":"ac0589f8-a6a5-4a91-989d-71f222e6502d"},"source":["- **NOTE:** the `random_state` parameter is here to set a reproducible output\n","- In practice, leave `random_state` out, with its default value of `None`\n","- Let's take a look at the first five elements of each of the variables returned by `make_blobs()`"]},{"cell_type":"code","execution_count":null,"id":"722de255","metadata":{"id":"722de255"},"outputs":[],"source":["features[:5]\n","true_labels[:5]"]},{"cell_type":"markdown","id":"5018838c","metadata":{"id":"5018838c"},"source":["- Now, let scale the values  for each feature in the dataset."]},{"cell_type":"code","execution_count":null,"id":"67bfde3a","metadata":{"id":"67bfde3a"},"outputs":[],"source":["# Scale the values for each feature in the dataset\n","scalar = StandardScaler()\n","scaled_features = scalar.fit_transform(features)\n","scaled_features[:5]"]},{"cell_type":"markdown","id":"769e0157","metadata":{"id":"769e0157"},"source":["- Instantiate a sample K-Means class"]},{"cell_type":"code","execution_count":null,"id":"19906fcd","metadata":{"id":"19906fcd"},"outputs":[],"source":["# Instantiate a sample K-Means class\n","kmeans = KMeans(init=\"random\", n_clusters=3, n_init=10, max_iter=300, random_state=42)"]},{"cell_type":"markdown","id":"e27f8337","metadata":{"id":"e27f8337"},"source":["- Fit the k-means class to the data in scaled_features, and analyze their parameters."]},{"cell_type":"code","execution_count":null,"id":"45786263","metadata":{"id":"45786263"},"outputs":[],"source":["# Fit the k-means class to the data in scaled_features\n","kmeans.fit(scaled_features)\n","\n","\n","# The lowest SSE value\n","kmeans.inertia_\n","\n","# Final locations of the centroid\n","kmeans.cluster_centers_\n","\n","# The number of iterations required to converge\n","kmeans.n_iter_\n","\n","# The first five predicted labels\n","kmeans.labels_[:5]"]},{"cell_type":"markdown","id":"95949cd6-3435-4907-b780-bb0828ec0da0","metadata":{"id":"95949cd6-3435-4907-b780-bb0828ec0da0"},"source":["## Implementation of K-Means Clustering"]},{"cell_type":"markdown","id":"e54e98b6-0bcc-497f-8110-ec48fbbe2e65","metadata":{"id":"e54e98b6-0bcc-497f-8110-ec48fbbe2e65"},"source":["- **Initialization**: Initialize the cluster centers randomly and create an empty list of points for each cluster.\n","- **E-step (Expectation)**: Assign each data point to the nearest cluster center.\n","- **M-step (Maximization)**: Recalculate the cluster centers as the mean of the points assigned to each cluster.\n","- **Repeat**: Iterate the E-step and M-step until the cluster centers converge."]},{"cell_type":"code","execution_count":null,"id":"cb58e280-4504-4ae8-a332-414cf5fb833f","metadata":{"id":"cb58e280-4504-4ae8-a332-414cf5fb833f"},"outputs":[],"source":["k = 3\n","clusters = {}\n","np.random.seed(23)\n","\n","for idx in range(k):\n","    center = 2 * (2 * np.random.random((features.shape[1],)) - 1)\n","    points = []\n","    cluster = {\"center\": center, \"points\": []}\n","    clusters[idx] = cluster\n","\n","clusters"]},{"cell_type":"code","execution_count":null,"id":"0ff70d2d-6bc9-4b28-8d88-36871e78a400","metadata":{"id":"0ff70d2d-6bc9-4b28-8d88-36871e78a400"},"outputs":[],"source":["# Plot the blobs with the initialized centers\n","plt.scatter(features[:, 0], features[:, 1])\n","plt.grid(True)\n","for i in clusters:\n","    center = clusters[i][\"center\"]\n","    plt.scatter(center[0], center[1], marker=\"*\", c=\"red\")\n","plt.show()"]},{"cell_type":"markdown","id":"6e47b858","metadata":{"id":"6e47b858"},"source":["- **Euclidean distance** is a measure of the \"straight line\" distance between two points.\n","- In KMeans, it is used to measure the similarity (or dissimilarity) between two points in a multi-dimensional space."]},{"cell_type":"code","execution_count":null,"id":"25708c6d-2981-4e25-bde9-10bb323a159b","metadata":{"id":"25708c6d-2981-4e25-bde9-10bb323a159b"},"outputs":[],"source":["# Define Euclidean distance\n","def distance(p1, p2):\n","    return np.sqrt(np.sum((p1 - p2) ** 2))"]},{"cell_type":"markdown","id":"106ab184","metadata":{"id":"106ab184"},"source":["- The **E-step** involves assigning each data point to the nearest cluster center using `assign_clusters` function.\n","- The **M-step** involves updating the cluster centers based on the points assigned to each cluster using `update_clusters` function.\n","- These steps are repeated iteratively until the cluster centers converge (i.e., they no longer change significantly between iterations)."]},{"cell_type":"code","execution_count":null,"id":"f7de7256-e490-4a51-9f1b-b06f09a33374","metadata":{"id":"f7de7256-e490-4a51-9f1b-b06f09a33374"},"outputs":[],"source":["# Implementing E-step\n","def assign_clusters(features, clusters):\n","    for idx in range(features.shape[0]):\n","        dist = []\n","\n","        curr_x = features[idx]\n","\n","        for i in range(k):\n","            dis = distance(curr_x, clusters[i][\"center\"])\n","            dist.append(dis)\n","        curr_cluster = np.argmin(dist)\n","        clusters[curr_cluster][\"points\"].append(curr_x)\n","    return clusters\n","\n","\n","# Implementing the M-Step\n","def update_clusters(features, clusters):\n","    for i in range(k):\n","        points = np.array(clusters[i][\"points\"])\n","        if points.shape[0] > 0:\n","            new_center = points.mean(axis=0)\n","            clusters[i][\"center\"] = new_center\n","\n","            clusters[i][\"points\"] = []\n","    return clusters\n","\n","\n","# Predict cluster for datapoints\n","def pred_cluster(features, clusters):\n","    pred = []\n","    for i in range(features.shape[0]):\n","        dist = []\n","        for j in range(k):\n","            dist.append(distance(features[i], clusters[j][\"center\"]))\n","        pred.append(np.argmin(dist))\n","    return pred\n","\n","\n","# Assign, update, and predict the cluster center\n","clusters = assign_clusters(features, clusters)\n","clusters = update_clusters(features, clusters)\n","pred = pred_cluster(features, clusters)"]},{"cell_type":"code","execution_count":null,"id":"bba81ad3-d527-40f4-872a-e68de1f13461","metadata":{"id":"bba81ad3-d527-40f4-872a-e68de1f13461"},"outputs":[],"source":["# Finally, plot the data points with their predicted cluster center\n","plt.scatter(features[:, 0], features[:, 1], c=pred)\n","for i in clusters:\n","    center = clusters[i][\"center\"]\n","    plt.scatter(center[0], center[1], marker=\"^\", c=\"red\")\n","plt.show()"]},{"cell_type":"markdown","id":"884ea3c8","metadata":{"id":"884ea3c8"},"source":["## Choosing the Number of Clusters"]},{"cell_type":"markdown","id":"fc795b42","metadata":{"id":"fc795b42"},"source":["- First lets find the optimal number of clusters (k) for K-means clustering using the **Elbow Method**."]},{"cell_type":"code","execution_count":null,"id":"2b2c361f","metadata":{"id":"2b2c361f"},"outputs":[],"source":["kmeans_kwargs = {\"init\": \"random\", \"n_init\": 10, \"max_iter\": 300, \"random_state\": 42}\n","\n","# save the SSE values for each k value\n","sse = []\n","for k in range(1, 11):\n","    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n","    kmeans.fit(scaled_features)\n","    sse.append(kmeans.inertia_)"]},{"cell_type":"code","execution_count":null,"id":"797436d1","metadata":{"id":"797436d1"},"outputs":[],"source":["# plot the number of clusters versus the SSE value to find/visualize the \"elbow point\"\n","plt.style.use(\"fivethirtyeight\")\n","plt.plot(range(1, 11), sse)\n","plt.xticks(range(1, 11))\n","plt.xlabel(\"Number of Clusters\")\n","plt.ylabel(\"SSE\")\n","plt.show()"]},{"cell_type":"markdown","id":"d029ae0a","metadata":{"id":"d029ae0a"},"source":["- The `KneeLocator` from the `kneed` library can be used to find the \"**elbow**\" point programmatically"]},{"cell_type":"code","execution_count":null,"id":"b8ea9e81","metadata":{"id":"b8ea9e81"},"outputs":[],"source":["# Use KneeLocator to identify the elbow point programmatically if needed\n","kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n","\n","kl.elbow"]},{"cell_type":"markdown","id":"84fbcddf","metadata":{"id":"84fbcddf"},"source":["- Next, lets find the optimal number of clusters (k) for K-means clustering using the **Silhouette Score**."]},{"cell_type":"code","execution_count":null,"id":"f6e75c2e","metadata":{"id":"f6e75c2e"},"outputs":[],"source":["# Find the silhouette coefficients and save them in a list for each k value\n","silhouette_coefficients = []\n","\n","for k in range(2, 11):\n","    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n","    kmeans.fit(scaled_features)\n","    score = silhouette_score(scaled_features, kmeans.labels_)\n","    silhouette_coefficients.append(score)"]},{"cell_type":"code","execution_count":null,"id":"3cdc366c","metadata":{"id":"3cdc366c"},"outputs":[],"source":["# Similarly plot the number of clusters versus the silhouette coefficient to find the optimal k value\n","plt.style.use(\"fivethirtyeight\")\n","plt.plot(range(2, 11), silhouette_coefficients)\n","plt.xticks(range(2, 11))\n","plt.xlabel(\"Number of Clusters\")\n","plt.ylabel(\"Silhouette Coefficient\")\n","plt.show()"]},{"cell_type":"markdown","id":"60faa09d","metadata":{"id":"60faa09d"},"source":["## K-means Clustering on a Real-World Dataset"]},{"cell_type":"markdown","id":"ea68e571-a5ad-469a-8f3e-18d95a0bdd02","metadata":{"id":"ea68e571-a5ad-469a-8f3e-18d95a0bdd02"},"source":["- Lets experiment with k-means clustering on a real-world dataset.\n","- We will be using the TCGA (The Cancer Genome Atlas) database which catalogs genomic alterations responsible for cancer."]},{"cell_type":"markdown","id":"ebbb8e9d-d296-4099-bc5d-c3e7d8422df6","metadata":{"id":"ebbb8e9d-d296-4099-bc5d-c3e7d8422df6"},"source":["- Import the necessary modules for extracting data and building/visualizing the data"]},{"cell_type":"code","execution_count":null,"id":"b2f20efe-b5f3-4e3c-80c3-4b62c9eacede","metadata":{"id":"b2f20efe-b5f3-4e3c-80c3-4b62c9eacede"},"outputs":[],"source":["uci_tcga_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00401/\"\n","archive_name = \"TCGA-PANCAN-HiSeq-801x20531.tar.gz\"\n","\n","# Build the url\n","full_download_url = urllib.parse.urljoin(uci_tcga_url, archive_name)\n","\n","# Download the file\n","r = urllib.request.urlretrieve(full_download_url, archive_name)\n","\n","# Extract the data from the archive\n","tar = tarfile.open(archive_name, \"r:gz\")\n","tar.extractall()\n","tar.close()"]},{"cell_type":"markdown","id":"6a7a654e","metadata":{"id":"6a7a654e"},"source":["- Load the **TCGA** data from file"]},{"cell_type":"code","execution_count":null,"id":"556151ab-5cee-4893-98a8-1a4d199fd038","metadata":{"id":"556151ab-5cee-4893-98a8-1a4d199fd038"},"outputs":[],"source":["# load the data from the text file as NumPy arrays\n","datafile = \"TCGA-PANCAN-HiSeq-801x20531/data.csv\"\n","labels_file = \"TCGA-PANCAN-HiSeq-801x20531/labels.csv\"\n","\n","data = np.genfromtxt(datafile, delimiter=\",\", usecols=range(1, 20532), skip_header=1)\n","\n","true_label_names = np.genfromtxt(\n","    labels_file, delimiter=\",\", usecols=(1,), skip_header=1, dtype=\"str\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"382adb3f-358c-4291-b316-182267983c02","metadata":{"id":"382adb3f-358c-4291-b316-182267983c02"},"outputs":[],"source":["# check the data and labels for the first five samples\n","data[:5, :3]\n","true_label_names[:5]"]},{"cell_type":"markdown","id":"0f13410e","metadata":{"id":"0f13410e"},"source":["- Convert the labels to integers to allow for usage in evaluation."]},{"cell_type":"code","execution_count":null,"id":"01a9712d-7696-4054-819e-6ba77cfbb3cc","metadata":{"id":"01a9712d-7696-4054-819e-6ba77cfbb3cc"},"outputs":[],"source":["# convert the labels to integers to allow for usage in evaluation\n","label_encoder = LabelEncoder()\n","\n","true_labels = label_encoder.fit_transform(true_label_names)\n","true_labels[:5]\n","\n","# store the length of the array of classes as the number of clusters\n","label_encoder.classes_\n","\n","n_clusters = len(label_encoder.classes_)"]},{"cell_type":"markdown","id":"3d4cb7ab","metadata":{"id":"3d4cb7ab"},"source":["- Build a preprocessing pipeline and use MinMaxScaler for feature scaling.\n","- Implement the PCA class to perform dimensionality reduction."]},{"cell_type":"code","execution_count":null,"id":"24231c70-e18f-4323-a1d3-d5b911cc46c0","metadata":{"id":"24231c70-e18f-4323-a1d3-d5b911cc46c0"},"outputs":[],"source":["# build a preprocessing pipeline and use MinMaxScaler for feature scaling, implement the PCA class to perform dimensionality reduction\n","preprocessor = Pipeline(\n","    [\n","        (\"scaler\", MinMaxScaler()),\n","        (\"pca\", PCA(n_components=2, random_state=42)),\n","    ]\n",")"]},{"cell_type":"markdown","id":"640de1bd","metadata":{"id":"640de1bd"},"source":["- Build another pipeline to perform k-means clustering."]},{"cell_type":"code","execution_count":null,"id":"b69ee90a-91e4-4239-9ac5-3734293983dc","metadata":{"id":"b69ee90a-91e4-4239-9ac5-3734293983dc"},"outputs":[],"source":["# Now build another pipeline to perform k-means clustering\n","clusterer = Pipeline(\n","    [\n","        (\n","            \"kmeans\",\n","            KMeans(\n","                n_clusters=n_clusters,\n","                init=\"k-means++\",\n","                n_init=50,\n","                max_iter=500,\n","                random_state=42,\n","            ),\n","        ),\n","    ]\n",")"]},{"cell_type":"markdown","id":"2b427907","metadata":{"id":"2b427907"},"source":["- Combine both the pipelines and train on the loaded dataset"]},{"cell_type":"code","execution_count":null,"id":"51e6ec23-7b7e-41a6-a2ea-aeb6233a9c49","metadata":{"id":"51e6ec23-7b7e-41a6-a2ea-aeb6233a9c49"},"outputs":[],"source":["# combine the two pipelines into one\n","pipe = Pipeline([(\"preprocessor\", preprocessor), (\"clusterer\", clusterer)])\n","\n","# use .fit() to perform all the pipeline steps on the data\n","pipe.fit(data)"]},{"cell_type":"markdown","id":"67eb2ef9","metadata":{"id":"67eb2ef9"},"source":["-  Evaluate performance by calculating the **Silhouette Score**."]},{"cell_type":"code","execution_count":null,"id":"e5c55011-1e6a-42f6-bf55-665491ec86ab","metadata":{"id":"e5c55011-1e6a-42f6-bf55-665491ec86ab"},"outputs":[],"source":["# calculate the silhouette score to evaluate performance\n","preprocessed_data = pipe[\"preprocessor\"].transform(data)\n","\n","predicted_labels = pipe[\"clusterer\"][\"kmeans\"].labels_\n","\n","silhouette_score(preprocessed_data, predicted_labels)\n","\n","# calculate the adjusted rand index (ARI)\n","adjusted_rand_score(true_labels, predicted_labels)"]},{"cell_type":"markdown","id":"c0bee3a9","metadata":{"id":"c0bee3a9"},"source":["- Visualise the results using graphs."]},{"cell_type":"code","execution_count":null,"id":"eb4cfc3b-b68a-4e5d-974b-68e842ec3bee","metadata":{"id":"eb4cfc3b-b68a-4e5d-974b-68e842ec3bee"},"outputs":[],"source":["# now visualize the data by plotting the results\n","pcadf = pd.DataFrame(\n","    pipe[\"preprocessor\"].transform(data),\n","    columns=[\"component_1\", \"component_2\"],\n",")\n","\n","pcadf[\"predicted_cluster\"] = pipe[\"clusterer\"][\"kmeans\"].labels_\n","pcadf[\"true_label\"] = label_encoder.inverse_transform(true_labels)\n","\n","plt.style.use(\"fivethirtyeight\")\n","plt.figure(figsize=(8, 8))\n","\n","scatter = sns.scatterplot(\n","    x=\"component_1\",\n","    y=\"component_2\",\n","    s=50,\n","    data=pcadf,\n","    hue=\"predicted_cluster\",\n","    style=\"true_label\",\n","    palette=\"Set2\",\n",")\n","\n","scatter.set_title(\"Clustering results from TCGA Pan-Cancer\\nGene Expression Data\")\n","plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n","plt.show()"]},{"cell_type":"markdown","id":"45456840-424f-49c7-89db-7f0a06c94725","metadata":{"id":"45456840-424f-49c7-89db-7f0a06c94725"},"source":["### Practical Coding Example:\n","\n","- In this section, first use `make_blobs` to create synthetic data.\n","- Then perform k-means clustering and plot the data.\n","- Then load the **iris** dataset from scikit-learn.\n","- Perform k-means clustering on the dataset, and then plot the clusters."]},{"cell_type":"code","execution_count":null,"id":"e063fa29-1d03-43a1-b1b8-062887c4f602","metadata":{"id":"e063fa29-1d03-43a1-b1b8-062887c4f602"},"outputs":[],"source":["# Import the necessary libraries\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs, load_iris\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"id":"7c965b5a-97cd-4e0e-8f09-4e05463dd227","metadata":{"id":"7c965b5a-97cd-4e0e-8f09-4e05463dd227"},"outputs":[],"source":["# Create synthetic data using make_blobs, fill in make_blobs\n","n_samples = 300\n","n_features = 2\n","n_clusters = 3\n","X, y = make_blobs()"]},{"cell_type":"code","execution_count":null,"id":"bf8b6ef9-b09b-4c8d-ae3c-4d37cf13ece9","metadata":{"id":"bf8b6ef9-b09b-4c8d-ae3c-4d37cf13ece9"},"outputs":[],"source":["# Plot the synthetic data\n"]},{"cell_type":"code","execution_count":null,"id":"5d982932-3eec-4f99-9c7a-8be64b487050","metadata":{"id":"5d982932-3eec-4f99-9c7a-8be64b487050"},"outputs":[],"source":["# Find and plot the optimal number of clusters using the elbow method\n","inertia = []\n","K = range(1, 11)\n","for k in K:\n"]},{"cell_type":"code","execution_count":null,"id":"e0d423fd-38e2-4e57-9e38-48eb47c1f21a","metadata":{"id":"e0d423fd-38e2-4e57-9e38-48eb47c1f21a"},"outputs":[],"source":["# Perform KMeans clustering on the synthetic data. using the k value determined visually from the previous plot\n"]},{"cell_type":"code","execution_count":null,"id":"7d24696a-8006-440b-9750-03bb99165a5f","metadata":{"id":"7d24696a-8006-440b-9750-03bb99165a5f"},"outputs":[],"source":["# Plot the clusters\n"]},{"cell_type":"code","execution_count":null,"id":"a4b7de07-0938-4d35-a5a8-4e88cfba2835","metadata":{"id":"a4b7de07-0938-4d35-a5a8-4e88cfba2835"},"outputs":[],"source":["# load the dataset load_iris, which will be used for this example\n","iris = load_iris()\n","X_iris = iris.data\n","y_iris = iris.target"]},{"cell_type":"code","execution_count":null,"id":"fb05f003-bad0-49e4-91d9-e1ebccf0cd50","metadata":{"id":"fb05f003-bad0-49e4-91d9-e1ebccf0cd50"},"outputs":[],"source":["# Standardize the Iris dataset\n","scaler = StandardScaler()\n","X_iris_scaled = scaler.fit_transform(X_iris)"]},{"cell_type":"code","execution_count":null,"id":"da64da3f-2249-44b2-ae2c-b3fcc03c383a","metadata":{"id":"da64da3f-2249-44b2-ae2c-b3fcc03c383a"},"outputs":[],"source":["# Find and plot the optimal number of clusters using the elbow method\n","inertia_iris = []\n","K_iris = range(1, 11)\n","for k in K_iris:"]},{"cell_type":"code","execution_count":null,"id":"5be291a6-207b-4791-976f-b329d53a222a","metadata":{"id":"5be291a6-207b-4791-976f-b329d53a222a"},"outputs":[],"source":["# Perform KMeans clustering on the Iris dataset\n"]},{"cell_type":"code","execution_count":null,"id":"a03316b1-7c9a-4b07-b306-44d6963fdab0","metadata":{"id":"a03316b1-7c9a-4b07-b306-44d6963fdab0"},"outputs":[],"source":["# Reduce dimensions for visualization using PCA\n"]},{"cell_type":"code","execution_count":null,"id":"fa116e36-5486-412f-8f1a-4b5d640971dd","metadata":{"id":"fa116e36-5486-412f-8f1a-4b5d640971dd"},"outputs":[],"source":["# Plot the Iris dataset clusters\n"]},{"cell_type":"markdown","id":"5b64d6f0-21ff-45d5-90ba-61dc431675ba","metadata":{"id":"5b64d6f0-21ff-45d5-90ba-61dc431675ba"},"source":["### Important information to remember"]},{"cell_type":"markdown","id":"93918640-518c-4293-aeee-36203d1b0035","metadata":{"id":"93918640-518c-4293-aeee-36203d1b0035"},"source":["1. What is the overall objective of k-means clustering?\n","    - The objective of K-Means clustering is to partition a dataset into k distinct, non-overlapping subsets, or 'clusters,' where the data points within each cluster are similar to each other and different from those in other clusters.\n","    - In a more technical sense, K-Means aims to minimize the within-cluster variance, which is typically measured using squared Euclidean distances.\n","\n","2. What are some situations where using K-Means is the best choice, and what are some where it isn't the best choice?\n","    1. Some situations where k-means is useful include:\n","        - data points form well-separated, spherical clusters\n","        - dataset has relatively low number of features, where Euclidean distance is more meaningful\n","        - dataset is large and/or clusters are expected to be similar sized\n","        - speed is needed, as k-means is a very fast clustering algorithm\n","\n","    2. Some situations where k-means is sub-optimal include:\n","        - clusters are irregularly shaped, as k-means assumes clusters are convex and spherical\n","        - clusters are different sizes, as k-means generally produces clusters of similar sizes\n","        - there are outliers, as k-means is sensitive to outliers, skewing the centroid positions\n","        - dataset has many dimensions/features, as the distance measures used by k-means can be less meaningful\n","\n","    3. Some alternative algorithms for sub-optimal situations:\n","        - DBSCAN for non-convex clusters and noisy data\n","        - Gaussian Mixture Models for clusters of different sizes and shapes\n","        - Other algorithms that best fit the specific characteristics of the dataset"]},{"cell_type":"markdown","id":"edd1d6f9-206c-4650-9811-caf15b0ef94e","metadata":{"id":"edd1d6f9-206c-4650-9811-caf15b0ef94e"},"source":["The primary goals of this notebook were to grasp the K-Means clustering algorithm and its functioning. With this knowledge, you should be able to implement your own K-Means clustering code and visualize the K-Means process. Hopefully, this notebook has achieved that and has enhanced your understanding of K-Means.\n","\n","### Resources\n","- https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n","- https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}